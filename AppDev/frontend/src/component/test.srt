1
00:00:13,020 --> 00:00:13,860
There is a day.

2
00:00:13,860 --> 00:00:16,280
About ten years ago when I asked a

3
00:00:16,280 --> 00:00:21,830
friend to hold a baby dinosaur
robot upside down.

4
00:00:21,830 --> 00:00:23,450
It was a toy called plea.

5
00:00:23,450 --> 00:00:25,660
All that he'd ordered and I was

6
00:00:25,660 --> 00:00:30,200
really excited about it because
I've always loved about this one

7
00:00:30,200 --> 00:00:31,800
has really caught technical
features.

8
00:00:31,800 --> 00:00:34,270
It had more orders and touch
sensors.

9
00:00:34,270 --> 00:00:38,390
It had an infra red camera and one
of the things that had was a tilt

10
00:00:38,390 --> 00:00:39,620
sensor so it.

11
00:00:39,620 --> 00:00:40,540
Knew what direction.

12
00:00:40,540 --> 00:00:41,680
It was facing.

13
00:00:41,680 --> 00:00:43,050
If and when you held it upside

14
00:00:43,050 --> 00:00:46,530
down.

15
00:00:46,530 --> 00:00:46,900
I thought.

16
00:00:46,900 --> 00:00:50,580
It's a super courts are showing
off to my friend and I said to

17
00:00:50,580 --> 00:00:55,220
hold it, but he'll see what debts.

18
00:00:55,220 --> 00:00:57,010
We were watching the theatrics of

19
00:00:57,010 --> 00:01:04,730
this robe that struggle and cry
out and and after a few seconds.

20
00:01:04,730 --> 00:01:05,420
The first.

21
00:01:05,420 --> 00:01:09,990
After my little and I said o.k.

22
00:01:09,990 --> 00:01:10,580
That's enough.

23
00:01:10,580 --> 00:01:14,640
Now, let's put him back down and

24
00:01:14,640 --> 00:01:15,670
pepper, about to make it.

25
00:01:15,670 --> 00:01:19,650
Stop crying and I was kind of a

26
00:01:19,650 --> 00:01:25,080
weird experience for me one thing,
wasn't the most maternal person at

27
00:01:25,080 --> 00:01:26,660
the time.

28
00:01:26,660 --> 00:01:27,670
Although, since then I've become a

29
00:01:27,670 --> 00:01:29,460
mother and nine months ago.

30
00:01:29,460 --> 00:01:31,530
And that is a score when hold them

31
00:01:31,530 --> 00:01:37,870
up to now, but my response to this
robot was also interesting because

32
00:01:37,870 --> 00:01:41,490
I knew exactly how this machine
work it.

33
00:01:41,490 --> 00:01:41,870
And yet.

34
00:01:41,870 --> 00:01:44,410
I still felt compelled to be kind

35
00:01:44,410 --> 00:01:46,410
to it.

36
00:01:46,410 --> 00:01:48,180
And that observation sparked that

37
00:01:48,180 --> 00:01:52,930
curiosity that I spent the fat,
the past decade pursuing it.

38
00:01:52,930 --> 00:01:56,260
Why did they comfort this robe.

39
00:01:56,260 --> 00:01:57,770
One of the things I discovered was

40
00:01:57,770 --> 00:02:01,600
that my treatment of this machine
was more than just an awkward

41
00:02:01,600 --> 00:02:05,460
moment in my living room that in a
world were increasingly

42
00:02:05,460 --> 00:02:10,389
integrating robots into our lives
and things like that might

43
00:02:10,389 --> 00:02:15,160
actually have consequences because
the first thing that I discovered

44
00:02:15,160 --> 00:02:15,560
is that.

45
00:02:15,560 --> 00:02:20,080
It's not just me in two thousand

46
00:02:20,080 --> 00:02:20,750
seven.

47
00:02:20,750 --> 00:02:22,420
The Washington Post reported that

48
00:02:22,420 --> 00:02:26,600
the United States military was
testing this robot diffused

49
00:02:26,600 --> 00:02:27,420
landmines.

50
00:02:27,420 --> 00:02:29,160
We workers were shaped like a

51
00:02:29,160 --> 00:02:33,290
stick insect would walk around a
minefield on its legs and every

52
00:02:33,290 --> 00:02:34,480
time he stepped on a mine.

53
00:02:34,480 --> 00:02:35,820
One of the legs would blow up

54
00:02:35,820 --> 00:02:40,160
would continue on the other legs
to block your minds in the colonel

55
00:02:40,160 --> 00:02:45,190
was in charge of this testing
exercise for calling it off

56
00:02:45,190 --> 00:02:50,090
because he says it's too inhumane
to watch this damage robot drag

57
00:02:50,090 --> 00:02:55,100
itself along the minefield.

58
00:02:55,100 --> 00:02:56,960
What would cause a hardened

59
00:02:56,960 --> 00:03:02,070
military officer and someone like
myself to have this response to

60
00:03:02,070 --> 00:03:02,250
row.

61
00:03:02,250 --> 00:03:04,070
But what.

62
00:03:04,070 --> 00:03:07,270
Of course for prime for science
fiction, pop culture really want

63
00:03:07,270 --> 00:03:12,250
to personify these things, but it
goes a little bit deeper than that

64
00:03:12,250 --> 00:03:16,100
it turns out that we are
biologically hard wired to project

65
00:03:16,100 --> 00:03:20,520
intent and life onto any movement
in a physical space.

66
00:03:20,520 --> 00:03:25,130
It seems I promised us some people
treat all sort of robots like

67
00:03:25,130 --> 00:03:26,650
their life.

68
00:03:26,650 --> 00:03:28,770
These bomb disposal units get

69
00:03:28,770 --> 00:03:29,440
names.

70
00:03:29,440 --> 00:03:31,400
They get medals of honour had

71
00:03:31,400 --> 00:03:34,850
funeral for them with gun salutes.

72
00:03:34,850 --> 00:03:36,090
Research shows that we do this.

73
00:03:36,090 --> 00:03:39,300
Even with very simple household
robots like the room.

74
00:03:39,300 --> 00:03:41,680
A vacuum cleaner.

75
00:03:41,680 --> 00:03:43,360
Just a desk that runs around the

76
00:03:43,360 --> 00:03:47,120
floor and clean it just the fact
that it's moving around on his own

77
00:03:47,120 --> 00:03:50,610
will cause people to name the
marimba and feel bad for the room.

78
00:03:50,610 --> 00:03:54,540
But when he gets stuck under the
couch.

79
00:03:54,540 --> 00:03:59,160
We can design about specifically
to invoke this response using eyes

80
00:03:59,160 --> 00:04:01,320
and faces were movement.

81
00:04:01,320 --> 00:04:02,530
People are magically

82
00:04:02,530 --> 00:04:06,580
subconsciously associate with
state of mind.

83
00:04:06,580 --> 00:04:09,360
There's an entire body of research
called Human robot interaction

84
00:04:09,360 --> 00:04:11,880
that really shows how all this
works so.

85
00:04:11,880 --> 00:04:12,580
For example.

86
00:04:12,580 --> 00:04:14,480
Researchers at Stamford University

87
00:04:14,480 --> 00:04:17,269
found out that makes people really
uncomfortable and asked them to

88
00:04:17,269 --> 00:04:23,390
touch her about his private parts
from this from any other studies.

89
00:04:23,390 --> 00:04:24,480
We know.

90
00:04:24,480 --> 00:04:26,650
We know that people respond to the

91
00:04:26,650 --> 00:04:29,900
cues given to them by the lifelike
machines.

92
00:04:29,900 --> 00:04:34,490
Even if they know that they're not
real.

93
00:04:34,490 --> 00:04:37,960
We're heading towards a world
where robots are everywhere about

94
00:04:37,960 --> 00:04:41,400
the technology is moving out from
behind factory was entering

95
00:04:41,400 --> 00:04:45,220
workplaces households and as these
machines.

96
00:04:45,220 --> 00:04:50,580
They can sense and make a ton of
my decisions and learn enter into

97
00:04:50,580 --> 00:04:52,680
the shared spaces.

98
00:04:52,680 --> 00:04:53,760
I think that maybe the best

99
00:04:53,760 --> 00:04:54,320
analogy.

100
00:04:54,320 --> 00:04:55,320
We have for this is our

101
00:04:55,320 --> 00:04:57,490
relationship with animals.

102
00:04:57,490 --> 00:04:59,760
Thousands of years ago, we started

103
00:04:59,760 --> 00:05:03,860
to domesticate animals and we
train them for work and weaponry

104
00:05:03,860 --> 00:05:05,640
and companionship.

105
00:05:05,640 --> 00:05:06,430
Throughout history.

106
00:05:06,430 --> 00:05:06,940
We've treated.

107
00:05:06,940 --> 00:05:09,170
Some animals like tools are the

108
00:05:09,170 --> 00:05:11,330
products and other animals.

109
00:05:11,330 --> 00:05:13,090
We treated with kindness and given

110
00:05:13,090 --> 00:05:15,810
a place in society as our
companions.

111
00:05:15,810 --> 00:05:16,700
I think it's possible.

112
00:05:16,700 --> 00:05:17,780
We might start to integrate

113
00:05:17,780 --> 00:05:24,570
Robartes, but similar weights
animals are alive.

114
00:05:24,570 --> 00:05:27,580
Robert and that.

115
00:05:27,580 --> 00:05:28,940
And I can tell you from working.

116
00:05:28,940 --> 00:05:32,040
What about the sister were pretty
far away from developing robots.

117
00:05:32,040 --> 00:05:37,770
They can feel anything there, but
we feel for that.

118
00:05:37,770 --> 00:05:41,550
And that matters because if we're
trying to integrate robots into

119
00:05:41,550 --> 00:05:44,780
the shared spaces need to
understand that people treat them

120
00:05:44,780 --> 00:05:49,190
differently than other devices
that in some cases.

121
00:05:49,190 --> 00:05:52,520
For example, the case of a soldier
who becomes emotionally attached

122
00:05:52,520 --> 00:05:53,140
to the robot.

123
00:05:53,140 --> 00:05:53,610
They work.

124
00:05:53,610 --> 00:05:58,500
Well, if that can be anything from
inefficient to dangerous.

125
00:05:58,500 --> 00:05:59,430
But in other cases.

126
00:05:59,430 --> 00:06:00,770
It can actually be used for the

127
00:06:00,770 --> 00:06:05,150
faster this emotional connection
to, but we're really seeing some

128
00:06:05,150 --> 00:06:06,260
great use cases.

129
00:06:06,260 --> 00:06:07,810
For example, robots working with

130
00:06:07,810 --> 00:06:11,180
autistic children to engage them
in ways that we haven't seen

131
00:06:11,180 --> 00:06:15,610
previously robot's working with
teachers to engage kids and learn

132
00:06:15,610 --> 00:06:21,070
with new results and it's not just
for kids early studies show that

133
00:06:21,070 --> 00:06:25,740
we can help doctors and patients
and health care settings and this

134
00:06:25,740 --> 00:06:29,160
is the pirate b. b. c. But it's
used in nursing homes with

135
00:06:29,160 --> 00:06:34,060
dementia patients has been around
for a while I remember years ago.

136
00:06:34,060 --> 00:06:38,540
Being a party and telling someone
about this throwback and her

137
00:06:38,540 --> 00:06:45,040
response was on my cart.

138
00:06:45,040 --> 00:06:46,170
I can't believe we're giving

139
00:06:46,170 --> 00:06:50,450
people robots instead of human
care.

140
00:06:50,450 --> 00:06:53,260
And this is a really common
response and I think it's

141
00:06:53,260 --> 00:06:57,440
absolutely correct because that
would be terrible.

142
00:06:57,440 --> 00:06:58,460
And in this case.

143
00:06:58,460 --> 00:06:59,970
It's not with this robot replace

144
00:06:59,970 --> 00:07:05,610
it with this robot replaces his
animal therapy in context which he

145
00:07:05,610 --> 00:07:07,230
was real animals.

146
00:07:07,230 --> 00:07:09,040
We can use robots because people

147
00:07:09,040 --> 00:07:11,720
consistently treat them like more.

148
00:07:11,720 --> 00:07:15,500
More like an animal and have it

149
00:07:15,500 --> 00:07:17,230
acknowledging this emotional
connection.

150
00:07:17,230 --> 00:07:20,280
Robert, can also help us
anticipate challenges as these

151
00:07:20,280 --> 00:07:20,870
devices.

152
00:07:20,870 --> 00:07:22,430
Move into more intimate areas of

153
00:07:22,430 --> 00:07:25,070
people's lives and for example is
it.

154
00:07:25,070 --> 00:07:25,500
o.k.

155
00:07:25,500 --> 00:07:27,540
If your child's teddy bear robot

156
00:07:27,540 --> 00:07:29,760
records private conversations.

157
00:07:29,760 --> 00:07:29,980
Is it.

158
00:07:29,980 --> 00:07:30,290
o.k.

159
00:07:30,290 --> 00:07:32,340
If your sex robot has compelling

160
00:07:32,340 --> 00:07:36,020
in our purchasers because rope.

161
00:07:36,020 --> 00:07:38,170
That's plus capitalism equals

162
00:07:38,170 --> 00:07:42,900
questions around consumer
protection and privacy and those

163
00:07:42,900 --> 00:07:45,650
aren't the only reason, said her
behaviour around these machines

164
00:07:45,650 --> 00:07:48,750
could, madam.

165
00:07:48,750 --> 00:07:50,700
A few years after that first

166
00:07:50,700 --> 00:07:51,710
initial experience.

167
00:07:51,710 --> 00:07:53,130
I had with this baby dinosaur

168
00:07:53,130 --> 00:07:56,200
robot do workshop with her friend
Hannah Scott.

169
00:07:56,200 --> 00:08:00,330
Scott, then we took five of these
baby dinosaur about we give them.

170
00:08:00,330 --> 00:08:02,310
The five teams of people.

171
00:08:02,310 --> 00:08:04,600
We had the name them and play with

172
00:08:04,600 --> 00:08:08,800
them and interact with them for
about an hour.

173
00:08:08,800 --> 00:08:11,610
Then we unveiled a him or a
hatchet and we told them to

174
00:08:11,610 --> 00:08:18,290
torture and kill the row and then
this turned out to be a little

175
00:08:18,290 --> 00:08:21,100
more dramatic than we expected it
to be because none of the

176
00:08:21,100 --> 00:08:24,210
participants wouldn't even so much
as straight.

177
00:08:24,210 --> 00:08:24,760
A robot.

178
00:08:24,760 --> 00:08:27,350
So we had to improvise.

179
00:08:27,350 --> 00:08:28,330
End at some point.

180
00:08:28,330 --> 00:08:30,070
He said o.k.

181
00:08:30,070 --> 00:08:32,110
You can save your team's robot.

182
00:08:32,110 --> 00:08:34,230
If you destroy another team throw.

183
00:08:34,230 --> 00:08:37,870
I I. And anyone that didn't work.

184
00:08:37,870 --> 00:08:38,610
They couldn't do it.

185
00:08:38,610 --> 00:08:42,500
So finally said, We're gonna
destroy all the robots are someone

186
00:08:42,500 --> 00:08:45,740
takes a hatchet to one of them.

187
00:08:45,740 --> 00:08:47,800
This guy stood up and he took the

188
00:08:47,800 --> 00:08:50,530
hatchet and the whole room,
Winston.

189
00:08:50,530 --> 00:08:54,690
See brother had to down on the
robot's neck and there was this

190
00:08:54,690 --> 00:08:56,020
half joking.

191
00:08:56,020 --> 00:08:58,810
Half serious moment of silence in

192
00:08:58,810 --> 00:09:05,370
the room from this farm and but so
that was a really interesting

193
00:09:05,370 --> 00:09:07,040
experience.

194
00:09:07,040 --> 00:09:08,940
It wasn't a controlled study up

195
00:09:08,940 --> 00:09:11,780
your sleeve, but it did lead to
some leader research that I did.

196
00:09:11,780 --> 00:09:13,270
i. t. With plush, London.

197
00:09:13,270 --> 00:09:13,930
Cynthia busy.

198
00:09:13,930 --> 00:09:18,600
Or we had people come into the lab
and smash these Hex bugs that move

199
00:09:18,600 --> 00:09:21,280
around in a really lifelike way,
like insects.

200
00:09:21,280 --> 00:09:22,810
So instead of choosing something
huge.

201
00:09:22,810 --> 00:09:26,590
People are trying to reach for
something more basic.

202
00:09:26,590 --> 00:09:30,780
And what we found was that high
embassy people would hesitate,

203
00:09:30,780 --> 00:09:33,620
more hit the heck's bucks.

204
00:09:33,620 --> 00:09:35,490
This is just a little study, but

205
00:09:35,490 --> 00:09:38,760
it's part of a large body of
research that is starting to

206
00:09:38,760 --> 00:09:41,300
indicate that there may be a
connection between people's

207
00:09:41,300 --> 00:09:45,660
tendencies for empathy and their
behaviour around, Rover.

208
00:09:45,660 --> 00:09:50,350
But my question for the coming era
of human robot interaction is not

209
00:09:50,350 --> 00:09:50,570
to.

210
00:09:50,570 --> 00:09:53,410
We empathise with throw, but it.

211
00:09:53,410 --> 00:09:58,490
Can robots change people's
thinking, Is there reason to.

212
00:09:58,490 --> 00:10:03,950
For example, prevent the child
from kicking about Doc That just

213
00:10:03,950 --> 00:10:07,600
out of respect for property
because the child may be more

214
00:10:07,600 --> 00:10:10,960
likely to take a real dark and
again.

215
00:10:10,960 --> 00:10:16,320
It's not just kids and this is the
violent video games question, but

216
00:10:16,320 --> 00:10:18,820
it's a completely new level
because of this visceral

217
00:10:18,820 --> 00:10:22,030
physicality that we respond more
intensely.

218
00:10:22,030 --> 00:10:28,120
Two images on a screen, we behave
violently towards Robarts

219
00:10:28,120 --> 00:10:32,620
specifically robots that are
designed to mimic life is that a

220
00:10:32,620 --> 00:10:37,020
healthy outlook from the behaviour
or is that training cruelty

221
00:10:37,020 --> 00:10:42,670
muscles.

222
00:10:42,670 --> 00:10:44,340
The answer to this question has

223
00:10:44,340 --> 00:10:47,960
the potential impact human
behaviour has the potential impact

224
00:10:47,960 --> 00:10:49,330
social norms.

225
00:10:49,330 --> 00:10:50,870
It has the potential to inspire

226
00:10:50,870 --> 00:10:51,800
rules around.

227
00:10:51,800 --> 00:10:53,630
What we can and can't do certain

228
00:10:53,630 --> 00:11:00,100
Robarts animal cruelty, but
because even if robots can't fuel

229
00:11:00,100 --> 00:11:06,060
our behaviour towards a matter for
us and regardless of whether we

230
00:11:06,060 --> 00:11:11,020
end up changing ovals robots might
be able to help us come to a new

231
00:11:11,020 --> 00:11:14,240
understanding of ourselves.

232
00:11:14,240 --> 00:11:15,530
Most of what learned over the past

233
00:11:15,530 --> 00:11:17,800
ten years have not been about
technology.

234
00:11:17,800 --> 00:11:23,260
A It's been about human psychology
and empathy and how we relate to

235
00:11:23,260 --> 00:11:25,110
others.

236
00:11:25,110 --> 00:11:27,000
And because when a child is kind

237
00:11:27,000 --> 00:11:27,500
to her room.

238
00:11:27,500 --> 00:11:30,870
But when a soldier tries to save a

239
00:11:30,870 --> 00:11:33,360
robot on the battlefield.

240
00:11:33,360 --> 00:11:35,000
When a group of people refuses to

241
00:11:35,000 --> 00:11:38,230
harm her about a baby dinosaur.

242
00:11:38,230 --> 00:11:40,050
Those robots aren't just motors in

243
00:11:40,050 --> 00:11:42,170
years and a groom's.

244
00:11:42,170 --> 00:11:43,630
Their reflections of our own

245
00:11:43,630 --> 00:11:44,340
humanity.

